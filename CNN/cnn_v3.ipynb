{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"..\"))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evlp_bronch.dataset import ALL_LUNG_IDS, RawEVLPDataset, ProcessedEVLPDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13aabacb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed):   \n",
    "    train_lung_ids, test_lung_ids = train_test_split(\n",
    "    ALL_LUNG_IDS, test_size=2, random_state=seed\n",
    "    )\n",
    "    train_lung_ids, val_lung_ids = train_test_split(\n",
    "    train_lung_ids, test_size=2, random_state=seed\n",
    "    )\n",
    "\n",
    "    train_dataset = ProcessedEVLPDataset(train_lung_ids)\n",
    "    val_dataset = ProcessedEVLPDataset(val_lung_ids)\n",
    "    test_dataset = ProcessedEVLPDataset(test_lung_ids)\n",
    "    print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 21:29:14,702 - evlp_bronch.dataset - INFO - Lung_id 29: Interpolation Dy_comp: between 329 and 331: 142.764474, 157.684626\n",
      "2023-12-11 21:29:14,727 - evlp_bronch.dataset - INFO - Lung_id 47: Interpolation Dy_comp: between 51 and 56: 50.582276, 72.674668\n",
      "2023-12-11 21:29:14,738 - evlp_bronch.dataset - INFO - Lung_id 53: Interpolation Dy_comp: between 2813 and 2824: 60.586801, 106.632301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 7 4\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = load_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last(lst, value): # find the last occurence of a value in a list\n",
    "    lst.reverse()\n",
    "    i = lst.index(value)\n",
    "    lst.reverse()\n",
    "    return len(lst) - i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_pad_sequence(sequence, target_length):\n",
    "    current_length = len(sequence)\n",
    "    total_padding = target_length - current_length\n",
    "    if total_padding <= 0:\n",
    "        return sequence\n",
    "    pad_after = total_padding\n",
    "\n",
    "    return np.pad(sequence, (0, pad_after), mode='edge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length_x():\n",
    "    m = [0,0,0]\n",
    "    for i in train_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[0]:\n",
    "            m[0]=metric_start+1\n",
    "    for i in val_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[1]:\n",
    "            m[1]=metric_start+1\n",
    "    for i in test_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[2]:\n",
    "            m[2]=metric_start+1\n",
    "    return max(m)\n",
    "max_l = find_max_length_x()\n",
    "max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length_y(dataset):\n",
    "    m = 0\n",
    "    for i in dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        changes = np.where(np.diff(i['Is_assessment']) == 1)[0]  # Find where each assessment period begins\n",
    "        # Find the first assessment period that starts after the last bronch occurrence\n",
    "        first_assessment_after_bronch = None\n",
    "        for change in changes:\n",
    "            if change > metric_start:\n",
    "                first_assessment_after_bronch = change\n",
    "                break\n",
    "        if metric_start< (len(i['Is_assessment']) - 1) * 0:\n",
    "            continue\n",
    "        if first_assessment_after_bronch is None:\n",
    "            first_assessment_after_bronch = len(i['Is_assessment']) - 1\n",
    "        if len(i['Dy_comp'][metric_start:first_assessment_after_bronch]) == 0: # if bronch紧接着assessment\n",
    "            continue\n",
    "        if len(i['Dy_comp'][metric_start:first_assessment_after_bronch]) > m:\n",
    "            m = first_assessment_after_bronch - metric_start\n",
    "    return m + 1\n",
    "max_y_train = find_max_length_y(train_dataset)\n",
    "max_y_val = find_max_length_y(val_dataset)\n",
    "max_y_test = find_max_length_y(test_dataset)\n",
    "max_y = max(max_y_train, max_y_val, max_y_test)\n",
    "max_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset(dataset, max_l, max_y):\n",
    "    X_dc = []\n",
    "    X_is_normal = []\n",
    "    X_is_bronch = []\n",
    "\n",
    "    Y = []\n",
    "    Y_len = []\n",
    "\n",
    "    for i in dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        changes = np.where(np.diff(i['Is_assessment']) == 1)[0]  # Find where each assessment period begins\n",
    "        # Find the first assessment period that starts after the last bronch occurrence\n",
    "        first_assessment_after_bronch = None\n",
    "        for change in changes:\n",
    "            if change > metric_start:\n",
    "                first_assessment_after_bronch = change\n",
    "                break\n",
    "        if metric_start< (len(i['Is_assessment']) - 1) * 0:\n",
    "            continue\n",
    "\n",
    "        if first_assessment_after_bronch is None:\n",
    "            first_assessment_after_bronch = len(i['Is_assessment']) - 1\n",
    "        if len(i['Dy_comp'][metric_start + 5:first_assessment_after_bronch]) == 0: # if bronch紧接着assessment\n",
    "            continue\n",
    "\n",
    "        # 做padding，保证长度一致，用最长的长度\n",
    "        X_dc.append(right_pad_sequence(i['Dy_comp'][:metric_start+1], max_l))\n",
    "        X_is_normal.append(right_pad_sequence(i['Is_normal'][:metric_start+1], max_l))\n",
    "        X_is_bronch.append(right_pad_sequence(i['Is_bronch'][:metric_start+1], max_l))\n",
    "\n",
    "        Y_len.append(len(i['Dy_comp'][metric_start + 5:first_assessment_after_bronch])) # 记录长度，用于计算loss)\n",
    "        Y.append(right_pad_sequence(i['Dy_comp'][metric_start + 5:first_assessment_after_bronch], max_y))\n",
    "\n",
    "    print(f\"length is {len(X_dc)}\")    \n",
    "    assert len(X_dc) == len(X_is_bronch) == len(X_is_normal) == len(Y), \"Inconsistent number of samples\"\n",
    "\n",
    "    X_dc = np.array(X_dc).reshape(-1, max_l)\n",
    "    X_is_normal = np.array(X_is_normal).reshape(-1, max_l)\n",
    "    X_is_bronch = np.array(X_is_bronch).reshape(-1, max_l)\n",
    "    Y = torch.from_numpy(np.array(Y)).float()\n",
    "    Y_len = torch.from_numpy(np.array(Y_len)).int()\n",
    "\n",
    "    X_combined = np.stack([X_dc, X_is_normal, X_is_bronch], axis=1)  # Shape becomes [N, 3, 1470]\n",
    "    X_combined = torch.from_numpy(X_combined).float()\n",
    "\n",
    "    return X_combined, Y, Y_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVLPDataset(Dataset):\n",
    "    def __init__(self, X_combined, Y, Y_len):\n",
    "        self.X_combined = X_combined\n",
    "        self.Y = Y\n",
    "        self.Y_len = Y_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_combined[idx], self.Y[idx], self.Y_len[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 142\n",
      "length is 7\n",
      "length is 4\n"
     ]
    }
   ],
   "source": [
    "x_combine_train, y_train, y_len_train = set_dataset(train_dataset, max_l, max_y)\n",
    "x_combine_val, y_val, y_len_val = set_dataset(val_dataset, max_l, max_y)\n",
    "x_combine_test, y_test, y_len_test = set_dataset(test_dataset, max_l, max_y)\n",
    "\n",
    "train_loader = DataLoader(EVLPDataset(x_combine_train, y_train, y_len_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(EVLPDataset(x_combine_val, y_val, y_len_val), batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(EVLPDataset(x_combine_test, y_test, y_len_test), batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs, plot=False):\n",
    "\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    val_losses = []\n",
    "    val_pearson_rs = []\n",
    "\n",
    "    val_target = []\n",
    "    val_predict = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, y, lengths in train_loader:\n",
    "            inputs, y = inputs.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            mask = torch.arange(outputs.size(1)).expand(len(lengths), outputs.size(1)) < lengths.unsqueeze(1)\n",
    "            mask = mask.to(device)\n",
    "            outputs_masked = torch.masked_select(outputs, mask).to(device)\n",
    "            y_masked = torch.masked_select(y, mask).to(device)\n",
    "\n",
    "            loss = criterion(outputs_masked, y_masked)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        epoch_losses.append(average_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = []\n",
    "            running_val_pearson_r = []\n",
    "            for inputs, y, lengths in val_loader:\n",
    "                inputs, y = inputs.to(device), y.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                mask = torch.arange(outputs.size(1)).expand(len(lengths), outputs.size(1)) < lengths.unsqueeze(1)\n",
    "                mask = mask.to(device)\n",
    "                outputs_masked = torch.masked_select(outputs, mask).to(device)\n",
    "                y_masked = torch.masked_select(y, mask).to(device)\n",
    "                val_loss = criterion(outputs_masked, y_masked)\n",
    "                running_val_loss.append(val_loss.cpu().item())\n",
    "                for i in range(outputs.size()[0]):\n",
    "                    val_pearson_r, _ = pearsonr(outputs[i, :lengths[i]].cpu().numpy(), y[i, :lengths[i]].cpu().numpy())\n",
    "                    running_val_pearson_r.append(val_pearson_r)\n",
    "                if epoch == epochs - 1:\n",
    "                    val_target.append(y[:, :lengths])\n",
    "                    val_predict.append(outputs[:, :lengths])\n",
    "\n",
    "            average_val_loss = np.mean(running_val_loss)\n",
    "            val_losses.append(average_val_loss)\n",
    "            average_val_peason_r = np.mean(running_val_pearson_r)\n",
    "            val_pearson_rs.append(average_val_peason_r)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Training Loss: {average_loss}, Validation Loss: {average_val_loss}, Validation Pearson's R: {average_val_peason_r}\")\n",
    "\n",
    "        if epoch == epochs - 1:\n",
    "            pearson_r_val = average_val_peason_r\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    # Plotting\n",
    "    if plot:\n",
    "        num_samples = len(val_predict)\n",
    "        cols = 2\n",
    "        rows = num_samples // cols + (num_samples % cols > 0)\n",
    "        plt.figure(figsize=(12, 4 * rows))\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.plot(val_predict[i][0].cpu().numpy(), label='Predicted')\n",
    "            plt.plot(val_target[i][0].cpu().numpy(), label='Target', alpha=0.7)\n",
    "            plt.title(f\"Sample {i+1}\")\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Values\")\n",
    "            plt.legend()\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.plot(range(1, epochs+1), epoch_losses, marker='o', color='blue', label='Training Loss')\n",
    "        plt.plot(range(1, epochs+1), val_losses, marker='o', color='red', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output_ls = []\n",
    "    target_ls = []\n",
    "\n",
    "    test_loss = 0\n",
    "    running_test_pearson_r = []\n",
    "    with torch.no_grad():\n",
    "        for data, y, lengths in test_loader:\n",
    "            data, y = data.to(device), y.to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            mask = torch.arange(outputs.size(1)).expand(len(lengths), outputs.size(1)) < lengths.unsqueeze(1)\n",
    "            mask = mask.to(device)\n",
    "            outputs_masked = torch.masked_select(outputs, mask).to(device)\n",
    "            y_masked = torch.masked_select(y, mask).to(device)\n",
    "\n",
    "            for i in range(outputs.size()[0]):\n",
    "                pearson_r, _ = pearsonr(outputs[i, :lengths[i]].cpu().numpy(), y[i, :lengths[i]].cpu().numpy())\n",
    "                running_test_pearson_r.append(pearson_r)\n",
    "\n",
    "            output_ls.append(outputs[:, :lengths])\n",
    "            target_ls.append(y[:, :lengths])\n",
    "\n",
    "            case_loss = criterion(outputs_masked, y_masked).item()\n",
    "            test_loss += case_loss\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    pearson_r_test = np.mean(running_test_pearson_r)\n",
    "\n",
    "    # Plotting\n",
    "    if plot:\n",
    "        num_samples = len(output_ls)\n",
    "        cols = 2\n",
    "        rows = num_samples // cols + (num_samples % cols > 0)\n",
    "        plt.figure(figsize=(12, 4 * rows))\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.plot(output_ls[i][0].cpu().numpy(), label='Predicted')\n",
    "            plt.plot(target_ls[i][0].cpu().numpy(), label='Target', alpha=0.7)\n",
    "            plt.title(f\"Sample {i+1}\")\n",
    "            plt.xlabel(\"Time Steps\")\n",
    "            plt.ylabel(\"Values\")\n",
    "            plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Pearson\\'s R: {pearson_r}')\n",
    "    return average_loss, average_val_loss, test_loss, pearson_r_val, pearson_r_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel=3, num_filters=64, num_in_channels=3, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(num_in_channels, num_filters, kernel_size=kernel, padding=padding)\n",
    "        self.conv1.weight.data.uniform_(0, 0.01)\n",
    "        self.conv2 = nn.Conv1d(num_filters, 128, kernel_size=kernel, padding=padding)\n",
    "        self.conv2.weight.data.uniform_(0, 0.01)\n",
    "\n",
    "        self.conv_seq = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv_output_size = self._calculate_conv_output_size(max_l, kernel, padding)\n",
    "        self.fc1 = torch.nn.Linear(conv_output_size, 128)\n",
    "        init.uniform_(self.fc1.weight, -0.01, 0.01)\n",
    "\n",
    "        self.fc_seq = torch.nn.Sequential( \n",
    "            self.fc1,\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(in_features=128, out_features=max_y)\n",
    "        init.uniform_(self.final_layer.weight, -0.01, 0.01)\n",
    "\n",
    "    def _calculate_conv_output_size(self, input_length, kernel, padding):\n",
    "        size = (input_length - kernel + 2 * padding) + 1\n",
    "        size = size // 2\n",
    "        size = (size - kernel + 2 * padding) + 1\n",
    "        size = size // 2\n",
    "        return size * 128  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_seq(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_seq(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# model = CNN(kernel=3, num_filters=64).to(device)\n",
    "# total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Total trainable parameters in the model: {total_params}\")\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000001, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# train(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, kernel=3, num_filters=64, num_in_channels=3, padding=0, lstm_hidden_size=128, lstm_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        conv1 = nn.Conv1d(num_in_channels, num_filters, kernel_size=kernel, padding=padding)\n",
    "        conv1.weight.data.uniform_(0, 0.01)\n",
    "        conv2 = nn.Conv1d(num_filters, 128, kernel_size=kernel, padding=padding)\n",
    "        conv2.weight.data.uniform_(0, 0.01)\n",
    "\n",
    "        self.conv_seq = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            conv2,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
    "\n",
    "        self.final_layer = nn.Linear(in_features=lstm_hidden_size, out_features=max_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_seq(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = nn.functional.leaky_relu(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "model1 = CNNLSTM(kernel=3, num_filters=64, lstm_hidden_size=128, lstm_layers=1).to(device)\n",
    "total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in the model: {total_params}\")\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01, amsgrad=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model1, train_loader, val_loader, test_loader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Regressive  (Abandoned)\n",
    "https://arxiv.org/pdf/1703.04122.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Offset Network (Multilayer Perceptron)\n",
    "class OffsetNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(OffsetNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc1.weight.data.uniform_(0.001, 0.005)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.leaky_relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# Define the Significance Network (Fully Convolutional Network)\n",
    "class SignificanceNetwork(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SignificanceNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(channels, max_y, kernel_size=3, dilation=1)\n",
    "        self.conv1.weight.data.uniform_(0.001, 0.005)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.leaky_relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "# Define the SOCNN Model\n",
    "class SOCNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, time_steps, channels):\n",
    "        super(SOCNN, self).__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "        self.significance_network = SignificanceNetwork(channels).to(device)\n",
    "        self.offset_network = OffsetNetwork(input_dim, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0] # [32, 3, 1416]\n",
    "        \n",
    "        self.W = nn.Parameter(torch.empty((batch_size, self.output_dim, self.time_steps)).uniform_(0, 0.005)).to(device) # [32, 368, 1416]\n",
    "        \n",
    "        significance = self.significance_network(x) # [32, 1416, 1414]\n",
    "\n",
    "        offsets = torch.stack([self.offset_network(x[:, :, i]) for i in range(self.time_steps)]) # [1416, 32, 1416]\n",
    "        offsets = torch.reshape(offsets, (offsets.shape[1], offsets.shape[0], offsets.shape[2])) # [32, 1416, 368]\n",
    "\n",
    "        temp = torch.bmm(self.W, offsets) # [32, 368, 368]\n",
    "        y_hat = torch.bmm(temp, significance) # [32, 368, 1414]\n",
    "        \n",
    "        y_hat = torch.sum(y_hat, dim=-1)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters in the model: 5152\n",
      "Epoch 20, Training Loss: 35459.036328125, Validation Loss: 341370.01785714284\n",
      "Epoch 40, Training Loss: 68819.5732421875, Validation Loss: 353839.02566964284\n",
      "Epoch 60, Training Loss: 243427.71943359374, Validation Loss: 287327.56870814733\n",
      "Epoch 80, Training Loss: 52285.768359375, Validation Loss: 261351.24441964287\n",
      "Epoch 100, Training Loss: 91496.0357421875, Validation Loss: 261265.5767299107\n",
      "Epoch 100, Pearson's R: 0.7527632184931585\n",
      "Test set: Average loss: 242172.6172, Pearson's R: 0.9236166957326347\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "model1 = SOCNN(input_dim=3, output_dim=max_y, time_steps=max_l, channels=3).to(device)\n",
    "total_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters in the model: {total_params}\")\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01, amsgrad=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model1, train_loader, val_loader, test_loader, criterion, optimizer, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
