{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"..\"))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evlp_bronch.dataset import ALL_LUNG_IDS, RawEVLPDataset, ProcessedEVLPDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.init as init\n",
    "\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x221b1215610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 21:17:11,719 - evlp_bronch.dataset - INFO - Lung_id 29: Interpolation Dy_comp: between 329 and 331: 142.764474, 157.684626\n",
      "2023-12-03 21:17:11,767 - evlp_bronch.dataset - INFO - Lung_id 47: Interpolation Dy_comp: between 51 and 56: 50.582276, 72.674668\n",
      "2023-12-03 21:17:11,798 - evlp_bronch.dataset - INFO - Lung_id 53: Interpolation Dy_comp: between 2813 and 2824: 60.586801, 106.632301\n"
     ]
    }
   ],
   "source": [
    "train_lung_ids, test_lung_ids = train_test_split(\n",
    "    ALL_LUNG_IDS, test_size=2, random_state=1\n",
    ")\n",
    "train_lung_ids, val_lung_ids = train_test_split(\n",
    "    train_lung_ids, test_size=2, random_state=1\n",
    ")\n",
    "\n",
    "train_dataset = ProcessedEVLPDataset(train_lung_ids)\n",
    "val_dataset = ProcessedEVLPDataset(val_lung_ids)\n",
    "test_dataset = ProcessedEVLPDataset(test_lung_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 7, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last(lst, value): # find the last occurence of a value in a list\n",
    "    lst.reverse()\n",
    "    i = lst.index(value)\n",
    "    lst.reverse()\n",
    "    return len(lst) - i - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def right_pad_sequence(sequence, target_length):\n",
    "    current_length = len(sequence)\n",
    "    total_padding = target_length - current_length\n",
    "    if total_padding <= 0:\n",
    "        return sequence\n",
    "    pad_after = total_padding\n",
    "\n",
    "    return np.pad(sequence, (0, pad_after), mode='edge')\n",
    "len(right_pad_sequence(train_dataset[0]['Dy_comp'][:1+1], 1415))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_length_x():\n",
    "    m = [0,0,0]\n",
    "    for i in train_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[0]:\n",
    "            m[0]=metric_start+1\n",
    "    for i in val_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[1]:\n",
    "            m[1]=metric_start+1\n",
    "    for i in test_dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        if metric_start > m[2]:\n",
    "            m[2]=metric_start+1\n",
    "    return max(m)\n",
    "max_l = find_max_length_x()\n",
    "max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(start_list, end_list, l):\n",
    "    y1 = np.mean(start_list, axis=0)\n",
    "    y2 = np.mean(end_list, axis=0)\n",
    "    slope = (y2 - y1) / l\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset(dataset):\n",
    "    X_dc = []\n",
    "    X_is_normal = []\n",
    "    X_is_bronch = []\n",
    "\n",
    "    Y = []\n",
    "\n",
    "    for i in dataset:\n",
    "        metric_start = find_last(list(i['Is_bronch']), 1) # find the last bronch\n",
    "        changes = np.where(np.diff(i['Is_assessment']) == 1)[0]  # Find where each assessment period begins\n",
    "        # Find the first assessment period that starts after the last bronch occurrence\n",
    "        first_assessment_after_bronch = None\n",
    "        for change in changes:\n",
    "            if change > metric_start:\n",
    "                first_assessment_after_bronch = change\n",
    "                break\n",
    "        if metric_start< (len(i['Is_assessment']) - 1) * 0:\n",
    "            continue\n",
    "\n",
    "        if first_assessment_after_bronch is None:\n",
    "            first_assessment_after_bronch = len(i['Is_assessment']) - 1\n",
    "        if len(i['Dy_comp'][metric_start:first_assessment_after_bronch]) == 0: # if bronch紧接着assessment\n",
    "            continue\n",
    "\n",
    "        # 做padding，保证长度一致，用最长的长度\n",
    "        X_dc.append(right_pad_sequence(i['Dy_comp'][:metric_start+1], max_l))\n",
    "        X_is_normal.append(right_pad_sequence(i['Is_normal'][:metric_start+1], max_l))\n",
    "        X_is_bronch.append(right_pad_sequence(i['Is_bronch'][:metric_start+1], max_l))\n",
    "        Y.append(calculate_slope(i['Dy_comp'][metric_start + 1: metric_start + 5], i['Dy_comp'][-4:-1], len(i['Dy_comp']) - metric_start - 1))\n",
    "\n",
    "    print(f\"length is {len(X_dc)}\")    \n",
    "    assert len(X_dc) == len(X_is_bronch) == len(X_is_normal) == len(Y), \"Inconsistent number of samples\"\n",
    "\n",
    "    X_dc = np.array(X_dc).reshape(-1, max_l)\n",
    "    X_is_normal = np.array(X_is_normal).reshape(-1, max_l)\n",
    "    X_is_bronch = np.array(X_is_bronch).reshape(-1, max_l)\n",
    "    Y = torch.from_numpy(np.array(Y)).float()\n",
    "\n",
    "    X_combined = np.stack([X_dc, X_is_normal, X_is_bronch], axis=1)  # Shape becomes [N, 3, 1470]\n",
    "    X_combined = torch.from_numpy(X_combined).float()\n",
    "\n",
    "    return X_combined, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVLPDataset(Dataset):\n",
    "    def __init__(self, X_combined, Y):\n",
    "        self.X_combined = X_combined\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_combined[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 143\n",
      "length is 7\n",
      "length is 4\n"
     ]
    }
   ],
   "source": [
    "x_combine_train, y_train = set_dataset(train_dataset)\n",
    "x_combine_val, y_val = set_dataset(val_dataset)\n",
    "x_combine_test, y_test = set_dataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(EVLPDataset(x_combine_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(EVLPDataset(x_combine_val, y_val), batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(EVLPDataset(x_combine_test, y_test), batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []  # List to store average training loss per epoch\n",
    "    val_losses = []    # List to store average validation loss per epoch\n",
    "\n",
    "    val_target = []\n",
    "    val_predict = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, y in train_loader:\n",
    "            inputs, y = inputs.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        epoch_losses.append(average_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_val_loss = 0.0\n",
    "            for inputs, y in val_loader:\n",
    "                inputs, y = inputs.to(device), y.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss = criterion(outputs, y)\n",
    "            \n",
    "                if epoch == epochs - 1:\n",
    "                    val_target.append(y)\n",
    "                    val_predict.append(outputs)\n",
    "    \n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "            average_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(average_val_loss)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Training Loss: {average_loss}, Validation Loss: {average_val_loss}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output_ls = []\n",
    "    target_ls = []\n",
    "\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, y in test_loader:\n",
    "            data, y = data.to(device), y.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            output_ls.append(outputs)\n",
    "            target_ls.append(y)\n",
    "\n",
    "            case_loss = criterion(outputs, y).item()\n",
    "            test_loss += case_loss\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, kernel=3, num_filters=64, num_in_channels=3, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(num_in_channels, num_filters, kernel_size=kernel, padding=padding)\n",
    "        self.conv1.weight.data.uniform_(0, 0.01)\n",
    "        self.conv2 = nn.Conv1d(num_filters, 128, kernel_size=kernel, padding=padding)\n",
    "        self.conv2.weight.data.uniform_(0, 0.01)\n",
    "\n",
    "        self.conv_seq = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv_output_size = self._calculate_conv_output_size(max_l, kernel, padding)\n",
    "        max_y = 1\n",
    "        self.fc1 = torch.nn.Linear(conv_output_size, 128)\n",
    "        init.uniform_(self.fc1.weight, -0.01, 0.01)\n",
    "\n",
    "        self.fc_seq = torch.nn.Sequential( \n",
    "            self.fc1,\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(in_features=128, out_features=max_y)\n",
    "        init.uniform_(self.final_layer.weight, -0.01, 0.01)\n",
    "\n",
    "    def _calculate_conv_output_size(self, input_length, kernel, padding):\n",
    "        size = (input_length - kernel + 2 * padding) + 1\n",
    "        size = size // 2\n",
    "        size = (size - kernel + 2 * padding) + 1\n",
    "        size = size // 2\n",
    "        return size * 128  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_seq(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_seq(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\interpreter\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\interpreter\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "d:\\interpreter\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Training Loss: 0.011284404690377415, Validation Loss: 0.0020649744879587422\n",
      "Epoch 40, Training Loss: 0.01194277296308428, Validation Loss: 0.0018668715607158706\n",
      "Epoch 60, Training Loss: 0.011357122007757426, Validation Loss: 0.0017704091782694117\n",
      "Epoch 80, Training Loss: 0.011946255271323026, Validation Loss: 0.0018928102768508584\n",
      "Epoch 100, Training Loss: 0.011590041918680072, Validation Loss: 0.00205516440577672\n",
      "Epoch 120, Training Loss: 0.01133207492530346, Validation Loss: 0.0017770495941087055\n",
      "Epoch 140, Training Loss: 0.011733517423272133, Validation Loss: 0.0019614668978777316\n",
      "Epoch 160, Training Loss: 0.013275116588920355, Validation Loss: 0.0019082970746759592\n",
      "Epoch 180, Training Loss: 0.011272911075502634, Validation Loss: 0.0018145603098673746\n",
      "Epoch 200, Training Loss: 0.011474159918725491, Validation Loss: 0.0017742292244033056\n",
      "Test set: Average loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model = CNN(kernel=3, num_filters=64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000001, betas=(0.9, 0.999), eps=1e-08, amsgrad=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train(model, train_loader, val_loader, test_loader, criterion, optimizer, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
